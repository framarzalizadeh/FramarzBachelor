% first example chapter
% @author Thomas Lehmann
%
\chapter{Evaluation}
\section{Testkonzept und Testumgebung}

Ziel des Testkonzepts war es, die grundsätzliche Funktionsfähigkeit und Machbarkeit des entwickelten Prototyps zu überprüfen. Im Vordergrund stand die manuelle, funktionale Validierung der End-to-End-Prozesse für die beiden zentralen Use-Cases \glqq Rechnung erstellen\grqq{} und \glqq Dokument speichern\grqq{}. Die Tests sollten zeigen, dass die konzipierten Automatisierungen technisch lauffähig sind und dass die vom Nutzer eingegebenen Daten jeweils zu den erwarteten Ergebnissen führen. Neben der reinen Funktionsfähigkeit wurden Datenkonsistenz, korrekte Dokumentgenerierung, grundlegende Stabilität sowie das subjektive Reaktionsverhalten des Systems betrachtet.

Der Testumfang umfasste alle wesentlichen Komponenten der prototypischen Lösung: den WhatsApp-Chatbot einschließlich der zugrunde liegenden Orchestrierungsworkflows, den End-to-End-Prozess zur Erstellung einer Rechnung sowie den Prozess zur Ablage eines Dokuments in der strukturierten Ordnerhierarchie. Darüber hinaus wurden die Anzeige- und Abruffunktionen der Webanwendung \glqq ClientHub\grqq{} über die serverseitigen Backend-Funktionen einbezogen. Nicht Gegenstand des Testumfangs waren hingegen tiefergehende Sicherheits- und Penetrationstests, umfangreiche Last- und Stresstests bei hoher Nutzerzahl oder API-Auslastung sowie automatisierte Unit- oder Integrationstests, da der Schwerpunkt der Arbeit auf einem funktionalen Prototyp und nicht auf der produktiven Härtung des Systems lag.

Die Tests wurden in einer einfachen Entwicklungs- und Testumgebung durchgeführt, die der Zielarchitektur entspricht, jedoch ohne produktive Nutzung. Für die Webanwendung kam eine separate Entwicklungs- bzw.\ Testversion zum Einsatz, die ausschließlich der Anzeige und Prüfung der gespeicherten Daten diente. Die Automatisierungen wurden direkt in der Integrationsplattform manuell über Einzelläufe gestartet, eine Trennung in gesonderte Test- und Produktiv-Workspaces war nicht vorgesehen. Die Datenhaltung erfolgte in Tabellen mit eigens angelegten Testdaten, während die Dokumentenablage in separaten Testordnern innerhalb des Cloud-Speichers realisiert wurde. Für die Chats wurde ein eigener Test-Account verwendet, über den die Dialoge manuell durchgespielt wurden.

Als Testdaten kamen ausschließlich synthetische, manuell angelegte Datensätze zum Einsatz, um reale Personen- oder Firmendaten zu vermeiden. Es wurden mehrere Beispielkunden mit typischen Konstellationen erstellt, darunter Privatkunden ohne Umsatzsteuer-Identifikationsnummer sowie Firmenkunden mit Firmenname und optionaler Umsatzsteuer-ID. Auf dieser Basis wurden mehrere Beispielrechnungen mit ein bis drei Rechnungspositionen erzeugt, um die wesentlichen Varianten abzudecken. Fehlerhafte oder unvollständige Eingaben wurden nicht systematisch vorbereitet, sondern situativ getestet, etwa durch leere oder falsche Eingaben im Chat oder den Abbruch eines Dialogs in der Mitte, um die Stabilität des Dialogflusses zu prüfen.

Das Testvorgehen war bewusst manuell, explorativ und end-to-end ausgerichtet. Die Tests wurden parallel zur Entwicklung durchgeführt, indem die Automatisierungen jeweils einmal manuell gestartet, die vollständigen Dialoge im Chat durchlaufen und die Ergebnisse in Datenhaltung, Dokumentenablage und Weboberfläche überprüft wurden. Formale Testfalldefinitionen oder automatisierte Tests kamen nicht zum Einsatz; stattdessen wurden Abweichungen direkt in den Workflows korrigiert und die betreffenden Abläufe erneut ausgeführt. Jeder zentrale Use-Case wurde mehrfach getestet, typischerweise in zwei bis fünf Durchläufen, um eine reproduzierbare Funktionsfähigkeit sicherzustellen.

Ein Test galt als bestanden, wenn der jeweilige End-to-End-Prozess ohne Abbruch durchlief, das erwartete Ergebnis sichtbar erzeugt wurde und der Ablauf bei wiederholter Ausführung stabil blieb. Hierzu zählen insbesondere eine korrekt befüllte und generierte PDF-Rechnung, die Ablage von Dokumenten im vorgesehenen Ordner, die vollständige Speicherung der relevanten Daten in den Tabellen sowie die anschließende Anzeige in der Webanwendung. Kleinere Fehler oder Unstimmigkeiten wurden als Teil des iterativen Entwicklungsprozesses behandelt: Die Workflows wurden angepasst, erneut getestet und verbleibende Einschränkungen bei Bedarf als bewusste Limitationen des Prototyps dokumentiert.

\section{Funktionale Tests}

Die funktionalen Tests zielten darauf ab, die zentralen Use-Cases des Systems manuell und end-to-end zu überprüfen. Im Mittelpunkt standen die Kernprozesse Rechnungserstellung, Dokumentenablage und die Anzeige der gespeicherten Daten in der Webanwendung. Die Tests wurden jeweils über die vorgesehenen Benutzeroberflächen gestartet und bis zur Erzeugung der erwarteten Artefakte (PDF-Dokumente, Datensätze, Ordnerstrukturen) vollständig durchlaufen.

Der Use-Case \glqq Rechnung mit bestehendem Kunden und einer Position erstellen\grqq{} startete im Chatbot mit der Auswahl der Funktion \glqq Rechnung erstellen\grqq{}, der Auswahl eines vorhandenen Kunden und der Eingabe einer einzelnen Rechnungsposition. Erwartet wurde, dass in der Datenhaltung neue Einträge in den Tabellen für Rechnung und Rechnungsposition angelegt und mit dem bestehenden Kundendatensatz verknüpft werden und der Nutzer eine korrekt befüllte PDF-Rechnung im Chat erhält. Nach kleineren Korrekturen an der Datenzuordnung lief dieser Ablauf stabil und reproduzierbar. Ein zweiter Use-Case betrachtete die Erstellung einer Rechnung für einen neu anzulegenden Kunden mit mehreren Positionen. Hierbei wurden im Dialog zunächst die Kundendaten erfasst, anschließend mehrere Positionen eingegeben und schließlich eine vollständige PDF-Rechnung erzeugt. Erwartet wurden ein neuer Kundendatensatz sowie konsistent verknüpfte Rechnungs- und Positionsdatensätze; nach Anpassung der Dialoglogik zur Verarbeitung mehrerer Positionen funktionierte dieser Ablauf ebenfalls zuverlässig.

Der Use-Case \glqq Dokument speichern\grqq{} überprüfte die Ablage beliebiger Dateien anhand einer zuvor im Chat ausgewählten Struktur aus Jahr, Quartal und Monat. Aus Sicht der Funktionalität war gefordert, dass die Ordnerhierarchie im Speicherdienst automatisch erzeugt oder wiederverwendet wird, die Datei im passenden Zielordner abgelegt und ein Verweis auf das Dokument in der Datenhaltung gespeichert wird. Nach Korrektur einzelner fehlerhafter Pfadzuordnungen wurden diese Anforderungen erfüllt. Ein weiterer Testfall betraf die Anzeige von Rechnungen und Dokumenten in der Webanwendung \glqq ClientHub\grqq{}. Über die Weboberfläche wurden Kunden ausgewählt und die zugehörigen Rechnungen und Dokumente angezeigt; dabei sollten die in der Datenhaltung gespeicherten Informationen konsistent abgebildet und verlinkte PDF-Dokumente direkt geöffnet oder heruntergeladen werden können. Die Tests bestätigten, dass die Lesefunktionen der Web-App zuverlässig arbeiten, wobei der Fokus auf der Anzeige und nicht auf der Bearbeitung lag.

Neben den regulären Abläufen wurden auch Fehler- und Sonderfälle betrachtet. Im Use-Case \glqq Fehlerfall – ungültige Eingabe im Chat\grqq{} wurden etwa leere Eingaben, Texte anstelle erwarteter Zahlen und der Abbruch eines Dialogs in der Mitte getestet. Ziel war es, sicherzustellen, dass der Prozess nicht unkontrolliert abbricht, sondern der Dialog im aktuellen oder vorherigen Schritt verbleibt und der Nutzer zur erneuten Eingabe aufgefordert wird. Die Tests zeigten, dass einfache Eingabefehler durch die Dialoglogik und Zustandsverwaltung abgefangen werden und der Nutzer in vielen Fällen sinnvoll weitergeführt wird; komplexere Sonderfälle werden im aktuellen Prototyp jedoch noch nicht in allen Varianten vollständig validiert und können in Einzelfällen zu suboptimalen Zuständen führen.

Insgesamt lässt sich festhalten, dass die zentralen End-to-End-Prozesse funktional stabil und reproduzierbar arbeiten. Die Kernfunktionen zur Rechnungserstellung, zur strukturierten Ablage von Dokumenten und zur Anzeige der Daten im ClientHub sind umgesetzt und liefern die erwarteten Ergebnisse. Vollständig erfüllt sind damit insbesondere die Anforderungen an die grundlegende Funktionsfähigkeit und Datenkonsistenz der Hauptprozesse. Nur teilweise erfüllt sind hingegen eine umfassende Fehlerbehandlung aller Rand- und Sonderfälle sowie eine formale, testfallbasierte Dokumentation, die für einen prototypischen Entwicklungsstand jedoch nicht im Fokus stand.

\section{Performance-Analyse}

Ziel der Performance-Analyse war nicht, formale Antwortzeiten im Millisekundenbereich zu bestimmen, sondern zu überprüfen, ob die End-to-End-Prozesse für einzelne Nutzer vollständig durchführbar sind und ob die dabei entstehenden Wartezeiten subjektiv noch nachvollziehbar und akzeptabel erscheinen. Im Vordergrund stand damit eine praxisnahe Einschätzung der tatsächlichen Wartezeit im Chat sowie beim Dokumenten-Upload, nicht die technische Optimierung einzelner Schnittstellen oder Komponenten. Die Analyse betrachtet daher exemplarische Abläufe und bewertet deren Verhalten qualitativ aus Nutzersicht.

Untersucht wurden drei typische Szenarien. Im ersten Szenario wurde die Rechnungserstellung für einen neuen Kunden mit einer Rechnungsposition betrachtet: Vom Start der Funktion \glqq Rechnung erstellen\grqq{} im Chat bis zur Rückgabe der fertigen PDF-Rechnung lagen die beobachteten Durchlaufzeiten bei etwa drei bis vier Minuten. Ein zweites Szenario betrachtete die Rechnungserstellung für einen bereits bestehenden Kunden; hier verkürzte sich die Dauer auf etwa zwei bis drei Minuten, da der Schritt der Kundenerfassung entfiel. Im dritten Szenario wurde der Prozess \glqq Dokument speichern\grqq{} gemessen, bei dem der Nutzer Jahr, Quartal und Monat auswählt und anschließend eine Datei sendet. Die Zeit vom Start der Funktion bis zur Ablage der Datei im vorgesehenen Ordner lag hier bei etwa ein bis zwei Minuten. Für die Webanwendung \glqq ClientHub\grqq{} wurde keine formale Zeitmessung durchgeführt, da die Anzeige- und Klickvorgänge subjektiv sehr schnell ablaufen und keinen dominierenden Einfluss auf den Gesamtprozess haben.

Die Messungen erfolgten bewusst einfach, indem die Dauer mit Hilfe einer Uhr beziehungsweise groben Zeitabschätzung aus Sicht des Nutzers erfasst wurde. Pro Szenario wurden mehrere Durchläufe durchgeführt, insbesondere nach Anpassungen der Automatisierungen, um die Reproduzierbarkeit der beobachteten Zeiten zu prüfen. Technische Timing-Werkzeuge wie Browser-Entwicklertools oder detaillierte Logauswertungen kamen nicht zum Einsatz, da der Schwerpunkt auf der wahrgenommenen Performance in einer typischen Nutzungssituation lag.

Die beobachteten Zeiten wurden im Wesentlichen durch die Ausführung der Automatisierungsworkflows und die Antwortzeiten des Messaging-Kanals geprägt. Insbesondere die sequentielle Abarbeitung mehrerer Module in der Integrationsplattform, interne Wartezeiten zwischen einzelnen Schritten sowie das Auffinden und Setzen des jeweils korrekten Dialogpfads im Chat tragen zur Gesamtdauer bei. Andere Faktoren wie die Performance der Webanwendung oder die lokale Internetverbindung spielten im Vergleich eine untergeordnete Rolle. Auffällige Timeouts oder vollständige Abbrüche traten im Rahmen der Tests nicht auf; die Abläufe waren zwar teilweise langsam, wurden jedoch stets vollständig beendet, sodass keine explizite Timeout-Analyse erforderlich war.

Aus Nutzersicht zeigt sich ein differenziertes Bild. Grundsätzlich sind alle zentralen Funktionen nutzbar, da die Prozesse technisch korrekt durchlaufen und die erwarteten Ergebnisse – insbesondere Rechnungs-PDFs und abgelegte Dokumente – erzeugt werden. Gleichzeitig wird deutlich, dass der Use-Case \glqq Rechnung erstellen\grqq{} durch die Vielzahl einzelner Dialogschritte, insbesondere bei der Neuanlage eines Kunden, als vergleichsweise langwierig wahrgenommen werden kann. Die schrittweise Abfrage von Vorname, Nachname, Adresse, Postleitzahl usw. erhöht zwar die Datenqualität, verlängert aber den Prozess und birgt das Risiko, dass Nutzer den Vorgang vorzeitig abbrechen. Demgegenüber wirkt der Use-Case \glqq Dokument speichern\grqq{} deutlich effizienter, da nur wenige Rückfragen erforderlich sind und das Ergebnis schnell sichtbar wird. Insgesamt bestätigt die Analyse den grundsätzlichen Nutzen des Automatisierungsansatzes, macht aber gleichzeitig deutlich, dass aus Sicht der Nutzererfahrung insbesondere bei der Rechnungserstellung Optimierungspotenzial besteht, etwa durch die Reduktion sequentieller Schritte oder die weitere Straffung der Dialogführung.

\section{Vergleich: manueller vs. automatisierter Prozess}

Bei der betrachteten Zielgruppe, also kleinen Unternehmen und Selbstständigen ohne spezialisiertes Buchhaltungs- oder ERP-System, erfolgt die Rechnungserstellung typischerweise in einem weitgehend manuellen Ablauf. Zunächst werden die benötigten Informationen wie Kundendaten, Leistungsbeschreibung und Preise gesammelt, anschließend wird eine Word- oder Excel-Vorlage geöffnet und die Daten werden manuell eingetragen und formatiert. Die Vergabe der Rechnungsnummer, der Export als PDF sowie das Speichern und Versenden der Datei, etwa per E-Mail oder Messenger, erfolgen ebenfalls manuell. Dieser Ansatz ist mit einem hohen Zeitaufwand verbunden, führt leicht zu Eingabe- und Formatierungsfehlern und resultiert häufig in einer unstrukturierten Ablage, wodurch Rechnungen später nur schwer auffindbar sind.

Das entwickelte System unterstützt und vereinfacht diesen Prozess, ohne ihn vollständig zu automatisieren. Durch die Nutzung eines einheitlichen Templates entfällt die manuelle Formatierung, und die PDF-Rechnung wird nach Abschluss der Datenerfassung automatisch erzeugt und an den Nutzer zurückgesendet. Rechnungsdaten und Dokumente werden zentral gespeichert, und die Datenerfassung erfolgt strukturiert über einen dialogbasierten Chat. Nicht automatisiert ist hingegen die Vergabe der Rechnungsnummer: Sie wird weiterhin vom Nutzer manuell eingegeben, ohne automatische Prüfung auf doppelte oder ungültige Nummern. Gleichzeitig entstehen neue Aufwände, da die Eingabe im Chat in mehrere Einzelschritte zerlegt ist; insbesondere bei der Neuanlage von Kunden werden zahlreiche Felder wie Name, Adresse, Postleitzahl und Ort nacheinander abgefragt, was den Prozess verlängert und als ermüdend empfunden werden kann.

Auch bei der Dokumentenablage zeigt sich ein deutlicher Unterschied zwischen manueller und automatisierter Vorgehensweise. In der Ausgangssituation werden Dateien gespeichert oder heruntergeladen, ein vermeintlich passender Ordner wird manuell gesucht oder neu angelegt und der Dateiname frei vergeben. Späteres Wiederfinden erfolgt über manuelle Suche, was durch uneinheitliche Ordner- und Dateinamen sowie fehlende zeitliche oder inhaltliche Struktur erschwert wird. Im entwickelten System werden Dokumente dagegen automatisch in einer fest definierten Ordnerstruktur nach Jahr, Quartal und Monat abgelegt; der zugehörige Link wird in der Datenhaltung gespeichert, sodass ein schneller Zugriff über Chat oder Webanwendung möglich ist. Der manuelle Aufwand bei der Ablage reduziert sich damit deutlich, auch wenn der Nutzer den Prozess weiterhin aktiv über die Funktion \glqq Dokument speichern\grqq{} starten und die Datei selbst bereitstellen muss.

Im Hinblick auf Zeit- und Aufwandsersparnis ergibt sich ein gemischtes Bild. Die manuelle Erstellung einer Rechnung nimmt, abhängig von Erfahrung und Vorlagenqualität, typischerweise etwa fünf bis fünfzehn Minuten in Anspruch. Mit dem Prototyp reduziert sich die Dauer bei bestehenden Kunden auf etwa zwei bis drei Minuten, bei neuen Kunden auf etwa drei bis vier Minuten, wobei die manuelle Eingabe der Rechnungsnummer enthalten ist. Die Dokumentenablage verkürzt sich von etwa zwei bis fünf Minuten bei rein manueller Vorgehensweise auf etwa ein bis zwei Minuten im automatisierten Prozess. Insgesamt verringert das System damit den Gesamtaufwand, insbesondere bei wiederkehrenden Vorgängen und bei der Ablage.

Hinsichtlich Fehleranfälligkeit und Transparenz bietet der automatisierte Ansatz ebenfalls Vorteile. Durch die zentrale Nutzung eines Templates werden Formatierungsfehler reduziert, und die schrittweise, strukturierte Datenerfassung senkt das Risiko vergessener Pflichtfelder. Gleichzeitig bleiben Fehler bei der Rechnungsnummer möglich, da diese weiterhin manuell vergeben wird und keine automatisierte Prüfung erfolgt. Positiv wirkt sich die zentrale Datenhaltung aus, die eine klare Verknüpfung zwischen Kunden, Rechnungen und Dokumenten ermöglicht und durch die einheitliche Ordnerstruktur die Nachvollziehbarkeit erhöht. Insgesamt zeigt der Vergleich, dass der Prototyp insbesondere bei Dokumentenerzeugung, Ablage und Datenstrukturierung einen klaren Mehrwert bietet, während die Rechnungserstellung zwar funktional nutzbar ist, aber durch lange Dialoge und verbleibende manuelle Schritte noch Optimierungspotenzial für einen produktiven Einsatz aufweist.

\section{Usability-Bewertung}

hier so tabelle machen
\section{Grenzen des Systems und Risiken}